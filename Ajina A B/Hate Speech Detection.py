# -*- coding: utf-8 -*-
"""Another copy of hate speech detection.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16HWpJSTlV9wxBU-bXTTIHmGvoBPP3ODG

## Hate speech detection

# Text preprocessing
"""

import pandas as pd
import re
import string

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the filename dynamically
filename = list(uploaded.keys())[0]

# Load the dataset into a DataFrame using the filename
df = pd.read_csv(filename)

# Display the first few rows
print(df.head())

# Function for text preprocessing
def preprocess(text):
    # 1. Lowercasing
    text = text.lower()

    # 2. Remove mentions (e.g., @username)
    text = re.sub(r'@\w+', '', text)

    # 3. Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)

    # 4. Remove punctuation and special characters
    text = text.translate(str.maketrans('', '', string.punctuation))

    # 5. Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Apply the preprocessing function to the 'tweet' column
df['processed_tweet'] = df['tweet'].apply(preprocess)

# Display the first few rows with the processed tweets
print(df[['tweet', 'processed_tweet']].head())

"""## **Class Distribution**"""

import pandas as pd
import re

# Read the CSV file from the specified path
file_path = "/content/train.csv"
data = pd.read_csv(file_path)


# Count the number of instances for each class
class_counts = data['class'].value_counts()

# Map the counts to descriptive labels
class_labels = {0: "Hate Speech", 1: "Offensive Language", 2: "Normal Speech"}
class_counts.index = class_counts.index.map(class_labels)

# Print the class distribution
print("Class Distribution:")
print(class_counts)

"""## **Data Visualization**"""

import pandas as pd
import re
import matplotlib.pyplot as plt

# Read the CSV file
file_path = "train.csv"  # Ensure the file is in the same directory
data = pd.read_csv(file_path)

# Preprocessing function
def preprocess_text_simple(text):
    # Convert to lowercase
    text = str(text).lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Remove mentions, hashtags, and special characters
    text = re.sub(r'@\w+|#\w+|[^a-z\s]', '', text)

    # Tokenize and remove extra spaces
    tokens = text.split()

    # Join tokens back into a single string
    cleaned_text = ' '.join(tokens)

    return cleaned_text

# Apply preprocessing to the 'tweet' column
data['cleaned_tweet'] = data['tweet'].apply(preprocess_text_simple)

# Count the number of instances for each class
class_counts = data['class'].value_counts()

# Map the counts to descriptive labels
class_labels = {0: "Hate Speech", 1: "Offensive Language", 2: "Normal Speech"}
class_counts.index = class_counts.index.map(class_labels)

# Print the class distribution
print("Class Distribution:")
print(class_counts)

# Plotting the bar graph for class distribution
plt.figure(figsize=(8, 6))
plt.bar(class_counts.index, class_counts.values, color=['red', 'orange', 'green'])
plt.xlabel("Class Type")
plt.ylabel("Number of Instances")
plt.title("Distribution of Classes in the Dataset")
plt.xticks(rotation=15)
plt.grid(axis='y')

# Display the plot
plt.show()

"""# **Dataset loading and Exploring**"""

import pandas as pd

# Load the uploaded dataset
file_path = '/content/cleaned train.csv'
data = pd.read_csv(file_path)

# Display basic information about the dataset
data.info(), data.head()

"""## Model Training and Evaluation"""

# Re-import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Extract features and target
X = data['cleaned_tweet'] # Changed df to data
y = data['class'] # Changed df to data

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert text data into TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train an SVM model with a linear kernel
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test_tfidf)

# Evaluate the model's performance
svm_accuracy = accuracy_score(y_test, y_pred)
svm_report = classification_report(y_test, y_pred)

svm_accuracy, svm_report
# Display raw accuracy and percentage
svm_accuracy_percentage = svm_accuracy * 100
print(f"Raw Accuracy: {svm_accuracy}")
print(f"Accuracy Percentage: {svm_accuracy_percentage:.2f} %")

"""# LinearSVC Model Tuning and Evaluation"""

import pandas as pd
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report

# Load the dataset
file_path = '/content/cleaned train.csv'
data = pd.read_csv(file_path)

# Select features and target
features = data[['count', 'hate_speech_count', 'offensive_language_count', 'neither_count']]
target = data['class']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Define the LinearSVC model
svc = LinearSVC(dual=False)  # dual=False for l1 penalty

# Define hyperparameter grid
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.01, 0.1, 1, 10, 100],
    'max_iter': [1000, 2000, 3000]
}

# Perform GridSearchCV
grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate on the test set
y_pred = best_model.predict(X_test)
report = classification_report(y_test, y_pred)

# Save the results
output_path = 'svm_hyperparameter_tuning_results.csv'
results = pd.DataFrame(grid_search.cv_results_)
results.to_csv(output_path, index=False)

# Print the best parameters and classification report
print("Best Parameters:", best_params)
print("Classification Report:\n", report)
print(f"Results saved to: {output_path}")